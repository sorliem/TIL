## Curating documents

This morning I found that our two most recent logstash indexes were _huge_.
I had turned on a different input source in LogStash yesterday, which was the source of the problem.
Consequently, the threshold had been reached where ES stops allocating shards and went into a yellow state.

I ended up switching off that input and learned how to delete a type of document within certain indexes.
Of course, ES does a soft delete when using the `-XDELETE` flag, so you have to call the `_optimize` API to 
remove the deleted documents from disk.

```bash
# Mark deleted docs of a certain type in a certain index
curl -XDELETE localhost:9200/logstash-2016.08.17/ELB/_query -d '{
  "query": {
    "bool": {
      "must": [
        { "match_all": {} }
      ]
    }
  }
}'

# Free memory of documents marked as DELETED
curl -XPOST 'http://localhost:9200/logstash-2016.08.17/_optimize?only_expunge_deletes'
```

Next thing I need to do is to set up a rotation system to delete indexes after the disk gets to a certain capacity.
[Curator](https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html) does just that
and I did a little test with some sample apache logs I had shoved into a local ES instance.

```bash
# Delete indexes when 0.002gb disk is used
curator --host 127.0.0.1 --port 9200 --dry-run delete --disk-space 0.002 indices --prefix logstash
```
